{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_pickle(paths.filename_data)\n",
    "\n",
    "#Get 3-class labels\n",
    "dict3 = {}\n",
    "dict3['0'] = '0'\n",
    "dict3['E'] = 'IE'\n",
    "dict3['S'] = 'IS'\n",
    "annotations = annotations.replace({\"label_3\": dict3})\n",
    "annotations['label'] = annotations['label_3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "#model specifics\n",
    "model_specifics = {\"data\": 'Reddit',\n",
    "    \"global_embedding_tp\": 'SBERT', #options: SBERT, BERT_cls , BERT_mean, BERT_max\n",
    "    \"dimensionality_reduction_tp\": 'umap', #options: ppapca, ppapcappa, umap\n",
    "    \"dimensionality_reduction_components\": 15, # options: any int number between 1 and embedding dimensions\n",
    "    \"dimensionality_reduction\": True, #options: True, False\n",
    "    \"time_injection_post_tp\": 'timestamp', #options: timestamp, None\n",
    "    \"signature_dimensions\": 3, #options: any int number larger than 1\n",
    "    \"post_embedding_tp\": 'sentence', #options: sentence, reduced, None\n",
    "    \"w\":5, #integer greater or equal to 2\n",
    "    \"pad_with\": 0, #options: any integer\n",
    "    \"loss_function\": 'focal', #options: focal, cbfocal\n",
    "    \"classifier_name\": 'SWNU-Network', #any string name\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6195, 384)\n"
     ]
    }
   ],
   "source": [
    "from utils.embeddings import Representations\n",
    "rep = Representations(type = model_specifics['global_embedding_tp'], filename =paths.filename_sbert)\n",
    "embeddings_sentence = rep.get_embeddings()\n",
    "\n",
    "print(embeddings_sentence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6195, 15)\n"
     ]
    }
   ],
   "source": [
    "from utils.dimensionality_reduction import DimensionalityReduction\n",
    "\n",
    "reduction = DimensionalityReduction(method= model_specifics['dimensionality_reduction_tp'], components=model_specifics['dimensionality_reduction_components'])\n",
    "embeddings_reduced = reduction.fit_transform(embeddings_sentence)\n",
    "\n",
    "print(embeddings_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenation with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import get_modeling_dataframe\n",
    "df = get_modeling_dataframe(annotations, embeddings_sentence, embeddings_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ttseriotou/signature_transforms/seq-sig-net/seq-sig-net/notebooks/../utils/timeinjection.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['timeline_index'][first_index:last_index] = np.arange(t_id_len)\n"
     ]
    }
   ],
   "source": [
    "from utils.timeinjection import TimeFeatures\n",
    "tf = TimeFeatures()\n",
    "df = tf.get_time_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6195, 400, 5])\n"
     ]
    }
   ],
   "source": [
    "from utils.preparedata import PrepareData\n",
    "\n",
    "getdata = PrepareData(model_specifics, time_column = 'time_encoding', zero_padding=True, w_last=True)\n",
    "df, df_padded = getdata.pad(df)\n",
    "x_data = getdata.unit_input(df, df_padded)\n",
    "print(x_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.classification_utils import Splits\n",
    "\n",
    "NUM_folds = 1\n",
    "splits = Splits(num_folds=NUM_folds)\n",
    "y_data = splits.get_labels(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit_SBERT_umap15_timestamp_sentence_SWNU-Network\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "#TRAINING PARAMETERS\n",
    "num_epochs = 100\n",
    "learning_rate =  [0.0003] \n",
    "gamma = [3] \n",
    "beta = None\n",
    "BATCH_SIZE = 64\n",
    "NUM_folds = 1\n",
    "patience = 3\n",
    "loss = 'focal'\n",
    "weight_decay_adam = 0.0001\n",
    "RANDOM_SEED_list = [0, 1, 12, 123, 1234]\n",
    "# ================================\n",
    "# MODEL PARAMETERS\n",
    "input_channels = model_specifics['dimensionality_reduction_components'] #15\n",
    "output_channels = [10]\n",
    "sig_d = 3\n",
    "hidden_dim_lstm = [[10]]\n",
    "embedding_dim = embeddings_sentence.shape[1] #384\n",
    "hidden_dim= [64]\n",
    "output_dim = y_data.unique().size()[0] #3\n",
    "dropout_rate= [0.1]\n",
    "augmentation_tp =  'Conv1d'\n",
    "augmentation_layers = ()\n",
    "comb_method ='concatenation'\n",
    "attention = False \n",
    "# ================================\n",
    "# MODEL OPTIONS\n",
    "save_results = False\n",
    "\n",
    "if (model_specifics['dimensionality_reduction'] == True):\n",
    "    model_code_name = model_specifics[\"data\"] + \"_\" + model_specifics[\"global_embedding_tp\"]  \\\n",
    "    + \"_\" + str(model_specifics['dimensionality_reduction_tp']) + str(model_specifics['dimensionality_reduction_components']) \\\n",
    "    + \"_\" + str(model_specifics['time_injection_post_tp']) \\\n",
    "    + \"_\" + str(model_specifics['post_embedding_tp'])  \\\n",
    "    + \"_\" + str(model_specifics['classifier_name'])\n",
    "else:\n",
    "    model_code_name = model_specifics[\"data\"] + \"_\" + model_specifics[\"global_embedding_tp\"]  \\\n",
    "    + \"_\" + str(model_specifics['time_injection_post_tp']) \\\n",
    "    + \"_\" + str(model_specifics['post_embedding_tp'])  \\\n",
    "    + \"_\" + str(model_specifics['classifier_name'])\n",
    "\n",
    "print(model_code_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting random seed # 0\n",
      "[0/100, 0/61] loss: 0.56981885\n",
      "Current Macro F1: 38.29265684032827\n",
      "Trigger Times: 0\n",
      "[1/100, 0/61] loss: 0.33743268\n",
      "Current Macro F1: 50.285628999458964\n",
      "Trigger Times: 0\n",
      "[2/100, 0/61] loss: 0.33874553\n",
      "Current Macro F1: 48.85506487817117\n",
      "Trigger Times: 1\n",
      "[3/100, 0/61] loss: 0.27603671\n",
      "Current Macro F1: 49.71341658661543\n",
      "Trigger Times: 0\n",
      "[4/100, 0/61] loss: 0.23250455\n",
      "Current Macro F1: 50.658869395711505\n",
      "Trigger Times: 0\n",
      "[5/100, 0/61] loss: 0.22305843\n",
      "Current Macro F1: 50.352310432231505\n",
      "Trigger Times: 1\n",
      "[6/100, 0/61] loss: 0.22916307\n",
      "Current Macro F1: 59.86108309572592\n",
      "Trigger Times: 0\n",
      "[7/100, 0/61] loss: 0.23767121\n",
      "Current Macro F1: 65.47869314028615\n",
      "Trigger Times: 0\n",
      "[8/100, 0/61] loss: 0.18360086\n",
      "Current Macro F1: 65.22610484589991\n",
      "Trigger Times: 1\n",
      "[9/100, 0/61] loss: 0.20629813\n",
      "Current Macro F1: 67.18706260596913\n",
      "Trigger Times: 0\n",
      "[10/100, 0/61] loss: 0.17864227\n",
      "Current Macro F1: 63.83577269376951\n",
      "Trigger Times: 1\n",
      "[11/100, 0/61] loss: 0.20426479\n",
      "Current Macro F1: 66.49771665082092\n",
      "Trigger Times: 0\n",
      "[12/100, 0/61] loss: 0.15201098\n",
      "Current Macro F1: 62.898591492696255\n",
      "Trigger Times: 1\n",
      "[13/100, 0/61] loss: 0.18206923\n",
      "Current Macro F1: 65.67749543531315\n",
      "Trigger Times: 0\n",
      "[14/100, 0/61] loss: 0.15950726\n",
      "Current Macro F1: 66.39835121613542\n",
      "Trigger Times: 0\n",
      "[15/100, 0/61] loss: 0.2178368\n",
      "Current Macro F1: 64.5992605029134\n",
      "Trigger Times: 1\n",
      "[16/100, 0/61] loss: 0.13725913\n",
      "Current Macro F1: 63.72104301276017\n",
      "Trigger Times: 2\n",
      "[17/100, 0/61] loss: 0.11310147\n",
      "Current Macro F1: 63.59473105976513\n",
      "Trigger Times: 3\n",
      "Early stopping!\n",
      "Starting random seed # 1\n",
      "[0/100, 0/61] loss: 0.52643973\n",
      "Current Macro F1: 30.46796256299496\n",
      "Trigger Times: 0\n",
      "[1/100, 0/61] loss: 0.43926954\n",
      "Current Macro F1: 50.72889002284667\n",
      "Trigger Times: 0\n",
      "[2/100, 0/61] loss: 0.46301439\n",
      "Current Macro F1: 49.67225851635425\n",
      "Trigger Times: 1\n",
      "[3/100, 0/61] loss: 0.25066772\n",
      "Current Macro F1: 48.647739993199544\n",
      "Trigger Times: 2\n",
      "[4/100, 0/61] loss: 0.20004138\n",
      "Current Macro F1: 53.590591917971174\n",
      "Trigger Times: 0\n",
      "[5/100, 0/61] loss: 0.25195378\n",
      "Current Macro F1: 52.783228700165786\n",
      "Trigger Times: 1\n",
      "[6/100, 0/61] loss: 0.30895403\n",
      "Current Macro F1: 54.012524973913045\n",
      "Trigger Times: 0\n",
      "[7/100, 0/61] loss: 0.24446921\n",
      "Current Macro F1: 60.426422515891176\n",
      "Trigger Times: 0\n",
      "[8/100, 0/61] loss: 0.33982632\n",
      "Current Macro F1: 65.67597443514266\n",
      "Trigger Times: 0\n",
      "[9/100, 0/61] loss: 0.13022159\n",
      "Current Macro F1: 65.56119907469724\n",
      "Trigger Times: 1\n",
      "[10/100, 0/61] loss: 0.24353492\n",
      "Current Macro F1: 67.26599663522123\n",
      "Trigger Times: 0\n",
      "[11/100, 0/61] loss: 0.22454207\n",
      "Current Macro F1: 62.07786340694877\n",
      "Trigger Times: 1\n",
      "[12/100, 0/61] loss: 0.21079879\n",
      "Current Macro F1: 67.79025044228217\n",
      "Trigger Times: 0\n",
      "[13/100, 0/61] loss: 0.14293358\n",
      "Current Macro F1: 64.57417136607725\n",
      "Trigger Times: 1\n",
      "[14/100, 0/61] loss: 0.22952692\n",
      "Current Macro F1: 64.27901265344586\n",
      "Trigger Times: 2\n",
      "[15/100, 0/61] loss: 0.18755685\n",
      "Current Macro F1: 64.60192694510768\n",
      "Trigger Times: 0\n",
      "[16/100, 0/61] loss: 0.15125245\n",
      "Current Macro F1: 66.4897246218586\n",
      "Trigger Times: 0\n",
      "[17/100, 0/61] loss: 0.13508052\n",
      "Current Macro F1: 64.48440209545538\n",
      "Trigger Times: 1\n",
      "[18/100, 0/61] loss: 0.14834985\n",
      "Current Macro F1: 65.0286838442682\n",
      "Trigger Times: 0\n",
      "[19/100, 0/61] loss: 0.18023986\n",
      "Current Macro F1: 63.32915023061568\n",
      "Trigger Times: 1\n",
      "[20/100, 0/61] loss: 0.15598905\n",
      "Current Macro F1: 66.38806060058432\n",
      "Trigger Times: 0\n",
      "[21/100, 0/61] loss: 0.16945456\n",
      "Current Macro F1: 64.52059334622123\n",
      "Trigger Times: 1\n",
      "[22/100, 0/61] loss: 0.11565361\n",
      "Current Macro F1: 63.746869428897035\n",
      "Trigger Times: 2\n",
      "[23/100, 0/61] loss: 0.10785447\n",
      "Current Macro F1: 61.918316803249496\n",
      "Trigger Times: 3\n",
      "Early stopping!\n",
      "Starting random seed # 12\n",
      "[0/100, 0/61] loss: 0.45954201\n",
      "Current Macro F1: 44.484978486104495\n",
      "Trigger Times: 0\n",
      "[1/100, 0/61] loss: 0.3634401\n",
      "Current Macro F1: 49.165094641780954\n",
      "Trigger Times: 0\n",
      "[2/100, 0/61] loss: 0.26302731\n",
      "Current Macro F1: 48.999248880555584\n",
      "Trigger Times: 1\n",
      "[3/100, 0/61] loss: 0.29903662\n",
      "Current Macro F1: 61.30924553246556\n",
      "Trigger Times: 0\n",
      "[4/100, 0/61] loss: 0.25763863\n",
      "Current Macro F1: 64.4678564529524\n",
      "Trigger Times: 0\n",
      "[5/100, 0/61] loss: 0.28279698\n",
      "Current Macro F1: 64.07956079008711\n",
      "Trigger Times: 1\n",
      "[6/100, 0/61] loss: 0.2605722\n",
      "Current Macro F1: 65.81598784943911\n",
      "Trigger Times: 0\n",
      "[7/100, 0/61] loss: 0.20252758\n",
      "Current Macro F1: 64.6881718585609\n",
      "Trigger Times: 1\n",
      "[8/100, 0/61] loss: 0.17700759\n",
      "Current Macro F1: 64.93925816539375\n",
      "Trigger Times: 0\n",
      "[9/100, 0/61] loss: 0.1443316\n",
      "Current Macro F1: 64.4594019799794\n",
      "Trigger Times: 1\n",
      "[10/100, 0/61] loss: 0.1232572\n",
      "Current Macro F1: 64.10750739543346\n",
      "Trigger Times: 2\n",
      "[11/100, 0/61] loss: 0.15606415\n",
      "Current Macro F1: 62.449642625081225\n",
      "Trigger Times: 3\n",
      "Early stopping!\n",
      "Starting random seed # 123\n",
      "[0/100, 0/61] loss: 0.49414733\n",
      "Current Macro F1: 30.46796256299496\n",
      "Trigger Times: 0\n",
      "[1/100, 0/61] loss: 0.50412673\n",
      "Current Macro F1: 49.37399756965549\n",
      "Trigger Times: 0\n",
      "[2/100, 0/61] loss: 0.30842012\n",
      "Current Macro F1: 48.51196574509753\n",
      "Trigger Times: 1\n",
      "[3/100, 0/61] loss: 0.23350982\n",
      "Current Macro F1: 49.07920101718551\n",
      "Trigger Times: 0\n",
      "[4/100, 0/61] loss: 0.38414565\n",
      "Current Macro F1: 49.64230060915143\n",
      "Trigger Times: 0\n",
      "[5/100, 0/61] loss: 0.3044098\n",
      "Current Macro F1: 51.78223564600679\n",
      "Trigger Times: 0\n",
      "[6/100, 0/61] loss: 0.24982066\n",
      "Current Macro F1: 64.40347987647765\n",
      "Trigger Times: 0\n",
      "[7/100, 0/61] loss: 0.34358466\n",
      "Current Macro F1: 57.23999425588647\n",
      "Trigger Times: 1\n",
      "[8/100, 0/61] loss: 0.18093875\n",
      "Current Macro F1: 61.016773799281545\n",
      "Trigger Times: 0\n",
      "[9/100, 0/61] loss: 0.27216563\n",
      "Current Macro F1: 60.145127593933054\n",
      "Trigger Times: 1\n",
      "[10/100, 0/61] loss: 0.24612963\n",
      "Current Macro F1: 65.418690926382\n",
      "Trigger Times: 0\n",
      "[11/100, 0/61] loss: 0.25600293\n",
      "Current Macro F1: 60.09631106708756\n",
      "Trigger Times: 1\n",
      "[12/100, 0/61] loss: 0.14329775\n",
      "Current Macro F1: 64.20327493855751\n",
      "Trigger Times: 0\n",
      "[13/100, 0/61] loss: 0.19430891\n",
      "Current Macro F1: 62.22534065137443\n",
      "Trigger Times: 1\n",
      "[14/100, 0/61] loss: 0.18543024\n",
      "Current Macro F1: 63.504229500274036\n",
      "Trigger Times: 0\n",
      "[15/100, 0/61] loss: 0.20568754\n",
      "Current Macro F1: 61.114616889802996\n",
      "Trigger Times: 1\n",
      "[16/100, 0/61] loss: 0.19102918\n",
      "Current Macro F1: 63.608152869150366\n",
      "Trigger Times: 0\n",
      "[17/100, 0/61] loss: 0.15948528\n",
      "Current Macro F1: 63.34269939705061\n",
      "Trigger Times: 1\n",
      "[18/100, 0/61] loss: 0.15561748\n",
      "Current Macro F1: 65.30785241248816\n",
      "Trigger Times: 0\n",
      "[19/100, 0/61] loss: 0.16727586\n",
      "Current Macro F1: 64.11920023934111\n",
      "Trigger Times: 1\n",
      "[20/100, 0/61] loss: 0.17732091\n",
      "Current Macro F1: 62.649872595024604\n",
      "Trigger Times: 2\n",
      "[21/100, 0/61] loss: 0.20696524\n",
      "Current Macro F1: 63.47868929530351\n",
      "Trigger Times: 0\n",
      "[22/100, 0/61] loss: 0.12751155\n",
      "Current Macro F1: 64.41869087076488\n",
      "Trigger Times: 0\n",
      "[23/100, 0/61] loss: 0.139944\n",
      "Current Macro F1: 63.14315408703164\n",
      "Trigger Times: 1\n",
      "[24/100, 0/61] loss: 0.15476514\n",
      "Current Macro F1: 60.89035341070209\n",
      "Trigger Times: 2\n",
      "[25/100, 0/61] loss: 0.17843226\n",
      "Current Macro F1: 64.16718726726623\n",
      "Trigger Times: 0\n",
      "[26/100, 0/61] loss: 0.19546109\n",
      "Current Macro F1: 63.3238596009717\n",
      "Trigger Times: 1\n",
      "[27/100, 0/61] loss: 0.071447313\n",
      "Current Macro F1: 60.60517665262816\n",
      "Trigger Times: 2\n",
      "[28/100, 0/61] loss: 0.12690054\n",
      "Current Macro F1: 61.18285163194973\n",
      "Trigger Times: 0\n",
      "[29/100, 0/61] loss: 0.12526496\n",
      "Current Macro F1: 61.332017247561296\n",
      "Trigger Times: 0\n",
      "[30/100, 0/61] loss: 0.061548624\n",
      "Current Macro F1: 62.42571362886774\n",
      "Trigger Times: 0\n",
      "[31/100, 0/61] loss: 0.080752775\n",
      "Current Macro F1: 62.71504774425921\n",
      "Trigger Times: 0\n",
      "[32/100, 0/61] loss: 0.096650027\n",
      "Current Macro F1: 62.03829022364741\n",
      "Trigger Times: 1\n",
      "[33/100, 0/61] loss: 0.094471768\n",
      "Current Macro F1: 60.79395129102988\n",
      "Trigger Times: 2\n",
      "[34/100, 0/61] loss: 0.13904253\n",
      "Current Macro F1: 60.002517061448735\n",
      "Trigger Times: 3\n",
      "Early stopping!\n",
      "Starting random seed # 1234\n",
      "[0/100, 0/61] loss: 0.55030346\n",
      "Current Macro F1: 41.988304093567244\n",
      "Trigger Times: 0\n",
      "[1/100, 0/61] loss: 0.31485221\n",
      "Current Macro F1: 49.4564810286102\n",
      "Trigger Times: 0\n",
      "[2/100, 0/61] loss: 0.27368337\n",
      "Current Macro F1: 49.93241413855973\n",
      "Trigger Times: 0\n",
      "[3/100, 0/61] loss: 0.1972447\n",
      "Current Macro F1: 61.19328236171118\n",
      "Trigger Times: 0\n",
      "[4/100, 0/61] loss: 0.26384804\n",
      "Current Macro F1: 62.485047846889955\n",
      "Trigger Times: 0\n",
      "[5/100, 0/61] loss: 0.23531827\n",
      "Current Macro F1: 65.34023438435203\n",
      "Trigger Times: 0\n",
      "[6/100, 0/61] loss: 0.17763808\n",
      "Current Macro F1: 64.01814081503854\n",
      "Trigger Times: 1\n",
      "[7/100, 0/61] loss: 0.33438632\n",
      "Current Macro F1: 64.16371285853437\n",
      "Trigger Times: 0\n",
      "[8/100, 0/61] loss: 0.2498149\n",
      "Current Macro F1: 63.37499869625688\n",
      "Trigger Times: 1\n",
      "[9/100, 0/61] loss: 0.19827574\n",
      "Current Macro F1: 62.82025879525913\n",
      "Trigger Times: 2\n",
      "[10/100, 0/61] loss: 0.21138103\n",
      "Current Macro F1: 60.13802625930106\n",
      "Trigger Times: 3\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from datetime import date\n",
    "from utils.classification_utils import set_seed, validation, training, testing\n",
    "from utils.loss_functions import FocalLoss, ClassBalanced_FocalLoss\n",
    "from models.swnunet import SWNUNet\n",
    "\n",
    "#Tuning over folds and random seeds\n",
    "ft_i = 0 #run number\n",
    "for out_ch in output_channels:\n",
    "    for lr in learning_rate:\n",
    "        for g in gamma:\n",
    "            for dp in dropout_rate:\n",
    "                for h_dim in hidden_dim:\n",
    "                    for lstm_dim in hidden_dim_lstm:\n",
    "                        #tuning parameters number\n",
    "                        str_version = 'tuning' + str(ft_i)\n",
    "                        ft_i+=1\n",
    "\n",
    "                        #dictionary of model parameters\n",
    "                        classifier_params = {\n",
    "                            \"num_epochs\": num_epochs,\n",
    "                            \"learning_rate\": lr,\n",
    "                            \"gamma\": g,\n",
    "                            \"beta\": beta,\n",
    "                            \"BATCH_SIZE\": BATCH_SIZE,\n",
    "                            \"NUM_folds\": NUM_folds,\n",
    "                            \"patience\": patience,\n",
    "                            \"loss\": loss,\n",
    "                            \"weight_decay_adam\": weight_decay_adam,\n",
    "                            \"RANDOM_SEED_list\": RANDOM_SEED_list,\n",
    "                            \"input_channels\": input_channels,\n",
    "                            \"output_channels\": out_ch,\n",
    "                            \"sig_d\": sig_d,\n",
    "                            \"hidden_dim_lstm\": lstm_dim,\n",
    "                            \"embedding_dimensions\": embedding_dim,\n",
    "                            \"hidden_dim\": h_dim,\n",
    "                            \"output_dim\": output_dim,\n",
    "                            \"dropout_rate\": dp,\n",
    "                            \"augmentation_tp\": augmentation_tp,\n",
    "                            \"augmentation_layers\": augmentation_layers,\n",
    "                            \"combination_method\": comb_method,\n",
    "                        }\n",
    "                                        \n",
    "                        for my_ran_seed in RANDOM_SEED_list:\n",
    "                            set_seed(my_ran_seed)\n",
    "                            myGenerator = torch.Generator()\n",
    "                            myGenerator.manual_seed(my_ran_seed)    \n",
    "                            for test_fold in range(NUM_folds):\n",
    "\n",
    "                                print('Starting random seed #',my_ran_seed)\n",
    "                                #get ith-fold data\n",
    "                                x_test, y_test, x_valid, y_valid, x_train , y_train, test_tl_ids, test_pids = Splits.get_reddit_splits(df, x_data, y_data)\n",
    "\n",
    "                                #data loaders with batches\n",
    "                                train = torch.utils.data.TensorDataset( x_train, y_train)\n",
    "                                valid = torch.utils.data.TensorDataset( x_valid, y_valid)\n",
    "                                test = torch.utils.data.TensorDataset( x_test, y_test)\n",
    "\n",
    "                                train_loader = torch.utils.data.DataLoader(dataset=train, batch_size = BATCH_SIZE, shuffle = True)\n",
    "                                valid_loader = torch.utils.data.DataLoader(dataset=valid, batch_size = BATCH_SIZE, shuffle = True)\n",
    "                                test_loader = torch.utils.data.DataLoader(dataset=test, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "                                #early stopping params\n",
    "                                last_metric = 0\n",
    "                                trigger_times = 0\n",
    "                                best_metric = 0\n",
    "\n",
    "                                #model\n",
    "                                model = SWNUNet(input_channels, \n",
    "                                            out_ch, \n",
    "                                            sig_d, \n",
    "                                            lstm_dim,\n",
    "                                            embedding_dim,\n",
    "                                            h_dim,\n",
    "                                            output_dim,\n",
    "                                            dp, \n",
    "                                            augmentation_tp,\n",
    "                                            augmentation_layers,\n",
    "                                            comb_method)\n",
    "\n",
    "                                #loss function\n",
    "                                if (loss=='focal') :\n",
    "                                    alpha_values = torch.Tensor([math.sqrt(1/(y_train[y_train==0].shape[0]/y_train.shape[0])), math.sqrt(1/(y_train[y_train==1].shape[0]/y_train.shape[0])), math.sqrt(1/(y_train[y_train==2].shape[0]/y_train.shape[0]))])\n",
    "                                    criterion = FocalLoss(gamma = g, alpha = alpha_values)\n",
    "                                elif (loss == 'cbfocal'):\n",
    "                                    classifier_params[\"beta\"] = beta\n",
    "                                    samples_count = torch.Tensor([y_train[y_train==0].shape[0], y_train[y_train==1].shape[0], y_train[y_train==2].shape[0]])\n",
    "                                    criterion = ClassBalanced_FocalLoss(gamma = g, beta = beta, no_of_classes=3, samples_per_cls=samples_count)   \n",
    "                                elif (loss == 'cross_entropy'):\n",
    "                                    criterion = nn.CrossEntropyLoss()                            \n",
    "                                optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay= weight_decay_adam)\n",
    "\n",
    "                                #model train/validation per epoch\n",
    "                                for epoch in range(num_epochs):\n",
    "\n",
    "                                    training(model, train_loader, criterion, optimizer, epoch, num_epochs)\n",
    "\n",
    "                                    # Early stopping\n",
    "                                    _ , f1_v, labels_val, predicted_val = validation(model, valid_loader, criterion, loss)\n",
    "\n",
    "                                    print('Current Macro F1:', f1_v)\n",
    "\n",
    "                                    if f1_v > best_metric :\n",
    "                                        best_metric = f1_v\n",
    "\n",
    "                                        #test and save so far best model\n",
    "                                        predicted_test, labels_test = testing(model, test_loader, loss)\n",
    "\n",
    "                                        results = {\n",
    "                                            \"model_code_name\": model_code_name, \n",
    "                                            \"model_specifics\": model_specifics, \n",
    "                                            \"classifier_params\": classifier_params, \n",
    "                                            \"date_run\": date.today().strftime(\"%d/%m/%Y\"),\n",
    "                                            \"test_tl_ids\": test_tl_ids,\n",
    "                                            \"test_pids\": test_pids,\n",
    "                                            \"labels\": labels_test,\n",
    "                                            \"predictions\": predicted_test,\n",
    "                                            \"labels_val\": labels_val,\n",
    "                                            \"predicted_val\": predicted_val,\n",
    "                                            \"test_fold\": test_fold,\n",
    "                                            \"random_seed\": my_ran_seed,\n",
    "                                            \"epoch\": epoch,\n",
    "                                        }\n",
    "\n",
    "                                        if (save_results==True):\n",
    "                                            file_name_results = paths.FOLDER_results + model_code_name + \"_\" + str(my_ran_seed) + \"seed\" + \"_\" + str_version + '.pkl'\n",
    "                                            file_name_model = paths.FOLDER_models + model_code_name + \"_\" + str(my_ran_seed) + \"seed\"  + \"_\" + str_version +'.pkl'\n",
    "                                            pickle.dump(results, open(file_name_results, 'wb'))\n",
    "                                            torch.save(model.state_dict(), file_name_model)\n",
    "\n",
    "                                    if f1_v < last_metric:\n",
    "                                        trigger_times += 1\n",
    "                                        print('Trigger Times:', trigger_times)\n",
    "\n",
    "                                        if trigger_times >= patience:\n",
    "                                            print('Early stopping!')\n",
    "                                            break\n",
    "\n",
    "                                    else:\n",
    "                                        print('Trigger Times: 0')\n",
    "                                        trigger_times = 0\n",
    "\n",
    "                                    last_metric = f1_v\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38-MoC",
   "language": "python",
   "name": "py38-moc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
