{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_pickle(paths.filename_data)\n",
    "\n",
    "#Get 3-class labels\n",
    "dict3 = {}\n",
    "dict3['0'] = '0'\n",
    "dict3['E'] = 'IE'\n",
    "dict3['S'] = 'IS'\n",
    "annotations = annotations.replace({\"label_3\": dict3})\n",
    "annotations['label'] = annotations['label_3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "#model specifics\n",
    "model_specifics = {\"data\": 'Reddit',\n",
    "    \"global_embedding_tp\": 'SBERT', #options: SBERT, BERT_cls , BERT_mean, BERT_max\n",
    "    \"dimensionality_reduction_tp\": None, #options: ppapca, ppapcappa, umap\n",
    "    \"dimensionality_reduction_components\": None, # options: any int number between 1 and embedding dimensions\n",
    "    \"dimensionality_reduction\": False, #options: True, False\n",
    "    \"time_injection_post_tp\": None, #options: timestamp, None\n",
    "    \"post_embedding_tp\": None, #options: sentence, reduced, None\n",
    "    \"history_len\": 29, #options: greater than 1\n",
    "    \"pad_with\": 123, #options: any integer\n",
    "    \"loss_function\": 'focal', #options: focal, cbfocal\n",
    "    \"classifier_name\": 'BiLSTM-SBERT-hist', #any string name\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6195, 384)\n"
     ]
    }
   ],
   "source": [
    "from utils.embeddings import Representations\n",
    "rep = Representations(type = model_specifics['global_embedding_tp'], filename =paths.filename_sbert)\n",
    "embeddings_sentence = rep.get_embeddings()\n",
    "\n",
    "print(embeddings_sentence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenation with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import get_modeling_dataframe\n",
    "df = get_modeling_dataframe(annotations, embeddings_sentence, embeddings_reduced=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6195, 29, 384])\n"
     ]
    }
   ],
   "source": [
    "from utils.preparedata import PrepareData\n",
    "\n",
    "getdata = PrepareData(model_specifics, time_column = None, zero_padding=True, w_last=True)\n",
    "df, df_padded = getdata.pad(df)\n",
    "x_data = getdata.unit_input(df, df_padded, embeddings_lastdim=True)\n",
    "print(x_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.classification_utils import Splits\n",
    "\n",
    "NUM_folds = 1\n",
    "splits = Splits(num_folds=NUM_folds)\n",
    "y_data = splits.get_labels(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit_SBERT_None_None_BiLSTM-SBERT-hist\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "#TRAINING PARAMETERS\n",
    "num_epochs = 100\n",
    "learning_rate =  [0.001] \n",
    "gamma = [2] \n",
    "beta = None\n",
    "BATCH_SIZE = [32]\n",
    "NUM_folds = 1\n",
    "patience = 3\n",
    "loss = 'focal'\n",
    "RANDOM_SEED_list = [0, 1, 12, 123, 1234]\n",
    "# ================================\n",
    "# MODEL PARAMETERS\n",
    "embedding_dim = embeddings_sentence.shape[1] #384\n",
    "hidden_dim_lstm1 = [128]  \n",
    "hidden_dim_lstm2 = 124 \n",
    "output_dim = y_data.unique().size()[0] #3\n",
    "dropout_rate= [0.5]\n",
    "pad_with = model_specifics['pad_with']\n",
    "# ================================\n",
    "# MODEL OPTIONS\n",
    "save_results = False\n",
    "\n",
    "if (model_specifics['dimensionality_reduction'] == True):\n",
    "    model_code_name = model_specifics[\"data\"] + \"_\" + model_specifics[\"global_embedding_tp\"]  \\\n",
    "    + \"_\" + str(model_specifics['dimensionality_reduction_tp']) + str(model_specifics['dimensionality_reduction_components']) \\\n",
    "    + \"_\" + str(model_specifics['time_injection_post_tp']) \\\n",
    "    + \"_\" + str(model_specifics['post_embedding_tp'])  \\\n",
    "    + \"_\" + str(model_specifics['classifier_name'])\n",
    "else:\n",
    "    model_code_name = model_specifics[\"data\"] + \"_\" + model_specifics[\"global_embedding_tp\"]  \\\n",
    "    + \"_\" + str(model_specifics['time_injection_post_tp']) \\\n",
    "    + \"_\" + str(model_specifics['post_embedding_tp'])  \\\n",
    "    + \"_\" + str(model_specifics['classifier_name'])\n",
    "\n",
    "print(model_code_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting random seed # 0\n",
      "[0/100, 0/122] loss: 0.78892422\n",
      "[0/100, 100/122] loss: 0.42969629\n",
      "Current Macro F1: 67.78657789184103\n",
      "Trigger Times: 0\n",
      "[1/100, 0/122] loss: 0.28486356\n",
      "[1/100, 100/122] loss: 0.33344704\n",
      "Current Macro F1: 70.48932386696163\n",
      "Trigger Times: 0\n",
      "[2/100, 0/122] loss: 0.24133493\n",
      "[2/100, 100/122] loss: 0.29525751\n",
      "Current Macro F1: 66.2118247049754\n",
      "Trigger Times: 1\n",
      "[3/100, 0/122] loss: 0.44636589\n",
      "[3/100, 100/122] loss: 0.29658687\n",
      "Current Macro F1: 67.96781361433807\n",
      "Trigger Times: 0\n",
      "[4/100, 0/122] loss: 0.32716483\n",
      "[4/100, 100/122] loss: 0.15448542\n",
      "Current Macro F1: 63.92250855543299\n",
      "Trigger Times: 1\n",
      "[5/100, 0/122] loss: 0.28753424\n",
      "[5/100, 100/122] loss: 0.33693406\n",
      "Current Macro F1: 65.89493245501716\n",
      "Trigger Times: 0\n",
      "[6/100, 0/122] loss: 0.19573054\n",
      "[6/100, 100/122] loss: 0.14748468\n",
      "Current Macro F1: 64.72607851918197\n",
      "Trigger Times: 1\n",
      "[7/100, 0/122] loss: 0.2420492\n",
      "[7/100, 100/122] loss: 0.058346462\n",
      "Current Macro F1: 64.27346842254518\n",
      "Trigger Times: 2\n",
      "[8/100, 0/122] loss: 0.18880112\n",
      "[8/100, 100/122] loss: 0.13079377\n",
      "Current Macro F1: 61.84037853723453\n",
      "Trigger Times: 3\n",
      "Early stopping!\n",
      "Starting random seed # 1\n",
      "[0/100, 0/122] loss: 0.87494552\n",
      "[0/100, 100/122] loss: 0.30986148\n",
      "Current Macro F1: 67.1798556238275\n",
      "Trigger Times: 0\n",
      "[1/100, 0/122] loss: 0.35275361\n",
      "[1/100, 100/122] loss: 0.21923199\n",
      "Current Macro F1: 69.2258426659075\n",
      "Trigger Times: 0\n",
      "[2/100, 0/122] loss: 0.30473161\n",
      "[2/100, 100/122] loss: 0.31243092\n",
      "Current Macro F1: 61.96555624047547\n",
      "Trigger Times: 1\n",
      "[3/100, 0/122] loss: 0.23278232\n",
      "[3/100, 100/122] loss: 0.27393514\n",
      "Current Macro F1: 63.16817885687064\n",
      "Trigger Times: 0\n",
      "[4/100, 0/122] loss: 0.091803417\n",
      "[4/100, 100/122] loss: 0.29926702\n",
      "Current Macro F1: 64.55916204230329\n",
      "Trigger Times: 0\n",
      "[5/100, 0/122] loss: 0.14575696\n",
      "[5/100, 100/122] loss: 0.13274109\n",
      "Current Macro F1: 67.22240432779166\n",
      "Trigger Times: 0\n",
      "[6/100, 0/122] loss: 0.19487408\n",
      "[6/100, 100/122] loss: 0.10390271\n",
      "Current Macro F1: 67.30542944816115\n",
      "Trigger Times: 0\n",
      "[7/100, 0/122] loss: 0.12767869\n",
      "[7/100, 100/122] loss: 0.15612262\n",
      "Current Macro F1: 65.68741453175177\n",
      "Trigger Times: 1\n",
      "[8/100, 0/122] loss: 0.30879423\n",
      "[8/100, 100/122] loss: 0.086825296\n",
      "Current Macro F1: 64.08848887702663\n",
      "Trigger Times: 2\n",
      "[9/100, 0/122] loss: 0.075867787\n",
      "[9/100, 100/122] loss: 0.20124583\n",
      "Current Macro F1: 64.97424878583341\n",
      "Trigger Times: 0\n",
      "[10/100, 0/122] loss: 0.091150589\n",
      "[10/100, 100/122] loss: 0.029704029\n",
      "Current Macro F1: 63.025311472717135\n",
      "Trigger Times: 1\n",
      "[11/100, 0/122] loss: 0.11116646\n",
      "[11/100, 100/122] loss: 0.15109864\n",
      "Current Macro F1: 63.16734477618432\n",
      "Trigger Times: 0\n",
      "[12/100, 0/122] loss: 0.065959752\n",
      "[12/100, 100/122] loss: 0.092104435\n",
      "Current Macro F1: 64.635916374305\n",
      "Trigger Times: 0\n",
      "[13/100, 0/122] loss: 0.071256436\n",
      "[13/100, 100/122] loss: 0.050646719\n",
      "Current Macro F1: 65.33084192189035\n",
      "Trigger Times: 0\n",
      "[14/100, 0/122] loss: 0.03209351\n",
      "[14/100, 100/122] loss: 0.058837503\n",
      "Current Macro F1: 66.66667007379047\n",
      "Trigger Times: 0\n",
      "[15/100, 0/122] loss: 0.094373576\n",
      "[15/100, 100/122] loss: 0.044407025\n",
      "Current Macro F1: 63.485694677485164\n",
      "Trigger Times: 1\n",
      "[16/100, 0/122] loss: 0.040941596\n",
      "[16/100, 100/122] loss: 0.019191936\n",
      "Current Macro F1: 65.20621468926552\n",
      "Trigger Times: 0\n",
      "[17/100, 0/122] loss: 0.066672981\n",
      "[17/100, 100/122] loss: 0.063337654\n",
      "Current Macro F1: 67.47544316118834\n",
      "Trigger Times: 0\n",
      "[18/100, 0/122] loss: 0.025354212\n",
      "[18/100, 100/122] loss: 0.014478266\n",
      "Current Macro F1: 63.72976260380842\n",
      "Trigger Times: 1\n",
      "[19/100, 0/122] loss: 0.064303376\n",
      "[19/100, 100/122] loss: 0.037046488\n",
      "Current Macro F1: 64.5659790611799\n",
      "Trigger Times: 0\n",
      "[20/100, 0/122] loss: 0.11235332\n",
      "[20/100, 100/122] loss: 0.018842291\n",
      "Current Macro F1: 64.55348902915979\n",
      "Trigger Times: 1\n",
      "[21/100, 0/122] loss: 0.010568355\n",
      "[21/100, 100/122] loss: 0.036696348\n",
      "Current Macro F1: 65.78789853043584\n",
      "Trigger Times: 0\n",
      "[22/100, 0/122] loss: 0.015393423\n",
      "[22/100, 100/122] loss: 0.0036914919\n",
      "Current Macro F1: 66.6741751107983\n",
      "Trigger Times: 0\n",
      "[23/100, 0/122] loss: 0.04246765\n",
      "[23/100, 100/122] loss: 0.003605278\n",
      "Current Macro F1: 63.87438706518931\n",
      "Trigger Times: 1\n",
      "[24/100, 0/122] loss: 0.01421212\n",
      "[24/100, 100/122] loss: 0.024990957\n",
      "Current Macro F1: 66.07007816465708\n",
      "Trigger Times: 0\n",
      "[25/100, 0/122] loss: 0.021533448\n",
      "[25/100, 100/122] loss: 0.015698291\n",
      "Current Macro F1: 64.24513025856811\n",
      "Trigger Times: 1\n",
      "[26/100, 0/122] loss: 0.022587357\n",
      "[26/100, 100/122] loss: 0.014884659\n",
      "Current Macro F1: 62.08133029825517\n",
      "Trigger Times: 2\n",
      "[27/100, 0/122] loss: 0.021669464\n",
      "[27/100, 100/122] loss: 0.0038440258\n",
      "Current Macro F1: 65.49313575351351\n",
      "Trigger Times: 0\n",
      "[28/100, 0/122] loss: 0.029276291\n",
      "[28/100, 100/122] loss: 0.015886376\n",
      "Current Macro F1: 64.8350827107383\n",
      "Trigger Times: 1\n",
      "[29/100, 0/122] loss: 0.0060983514\n",
      "[29/100, 100/122] loss: 0.0056152674\n",
      "Current Macro F1: 63.30360688059512\n",
      "Trigger Times: 2\n",
      "[30/100, 0/122] loss: 0.001439524\n",
      "[30/100, 100/122] loss: 0.018758496\n",
      "Current Macro F1: 65.25403806010985\n",
      "Trigger Times: 0\n",
      "[31/100, 0/122] loss: 0.034018103\n",
      "[31/100, 100/122] loss: 0.0051495791\n",
      "Current Macro F1: 63.96784671726108\n",
      "Trigger Times: 1\n",
      "[32/100, 0/122] loss: 0.0032464336\n",
      "[32/100, 100/122] loss: 0.018119071\n",
      "Current Macro F1: 66.13310363776979\n",
      "Trigger Times: 0\n",
      "[33/100, 0/122] loss: 0.015481625\n",
      "[33/100, 100/122] loss: 0.010443048\n",
      "Current Macro F1: 64.16667445868825\n",
      "Trigger Times: 1\n",
      "[34/100, 0/122] loss: 0.012541225\n",
      "[34/100, 100/122] loss: 0.0046407441\n",
      "Current Macro F1: 65.9341601800558\n",
      "Trigger Times: 0\n",
      "[35/100, 0/122] loss: 0.0025958451\n",
      "[35/100, 100/122] loss: 0.008946904\n",
      "Current Macro F1: 66.94354270560528\n",
      "Trigger Times: 0\n",
      "[36/100, 0/122] loss: 0.0018886395\n",
      "[36/100, 100/122] loss: 0.029442428\n",
      "Current Macro F1: 67.01157699813011\n",
      "Trigger Times: 0\n",
      "[37/100, 0/122] loss: 0.01598162\n",
      "[37/100, 100/122] loss: 0.060886145\n",
      "Current Macro F1: 67.35212103322668\n",
      "Trigger Times: 0\n",
      "[38/100, 0/122] loss: 0.035047408\n",
      "[38/100, 100/122] loss: 0.048207331\n",
      "Current Macro F1: 64.9423350386511\n",
      "Trigger Times: 1\n",
      "[39/100, 0/122] loss: 0.012109125\n",
      "[39/100, 100/122] loss: 0.0097337281\n",
      "Current Macro F1: 62.78747949368567\n",
      "Trigger Times: 2\n",
      "[40/100, 0/122] loss: 0.0061918111\n",
      "[40/100, 100/122] loss: 0.033377573\n",
      "Current Macro F1: 64.89276542934326\n",
      "Trigger Times: 0\n",
      "[41/100, 0/122] loss: 0.042395368\n",
      "[41/100, 100/122] loss: 0.023982404\n",
      "Current Macro F1: 66.0071689420754\n",
      "Trigger Times: 0\n",
      "[42/100, 0/122] loss: 0.0068326565\n",
      "[42/100, 100/122] loss: 0.014348403\n",
      "Current Macro F1: 65.95820680149303\n",
      "Trigger Times: 1\n",
      "[43/100, 0/122] loss: 0.021810815\n",
      "[43/100, 100/122] loss: 0.001770976\n",
      "Current Macro F1: 65.84840714031375\n",
      "Trigger Times: 2\n",
      "[44/100, 0/122] loss: 0.0069180098\n",
      "[44/100, 100/122] loss: 0.0019865208\n",
      "Current Macro F1: 65.51069678178311\n",
      "Trigger Times: 3\n",
      "Early stopping!\n",
      "Starting random seed # 12\n",
      "[0/100, 0/122] loss: 0.68772447\n",
      "[0/100, 100/122] loss: 0.67913651\n",
      "Current Macro F1: 62.83854183041754\n",
      "Trigger Times: 0\n",
      "[1/100, 0/122] loss: 0.4111678\n",
      "[1/100, 100/122] loss: 0.29990861\n",
      "Current Macro F1: 69.13328761478513\n",
      "Trigger Times: 0\n",
      "[2/100, 0/122] loss: 0.26096705\n",
      "[2/100, 100/122] loss: 0.19978428\n",
      "Current Macro F1: 67.60535591830926\n",
      "Trigger Times: 1\n",
      "[3/100, 0/122] loss: 0.20567328\n",
      "[3/100, 100/122] loss: 0.15288644\n",
      "Current Macro F1: 65.8055871562683\n",
      "Trigger Times: 2\n",
      "[4/100, 0/122] loss: 0.18883355\n",
      "[4/100, 100/122] loss: 0.54251951\n",
      "Current Macro F1: 67.93303344765627\n",
      "Trigger Times: 0\n",
      "[5/100, 0/122] loss: 0.17559573\n",
      "[5/100, 100/122] loss: 0.23864745\n",
      "Current Macro F1: 66.45331402568559\n",
      "Trigger Times: 1\n",
      "[6/100, 0/122] loss: 0.36762476\n",
      "[6/100, 100/122] loss: 0.1281184\n",
      "Current Macro F1: 64.79906538773493\n",
      "Trigger Times: 2\n",
      "[7/100, 0/122] loss: 0.11931214\n",
      "[7/100, 100/122] loss: 0.084550194\n",
      "Current Macro F1: 66.01277966911304\n",
      "Trigger Times: 0\n",
      "[8/100, 0/122] loss: 0.17296642\n",
      "[8/100, 100/122] loss: 0.098692171\n",
      "Current Macro F1: 64.51517090298188\n",
      "Trigger Times: 1\n",
      "[9/100, 0/122] loss: 0.099173099\n",
      "[9/100, 100/122] loss: 0.15670009\n",
      "Current Macro F1: 65.98521790403848\n",
      "Trigger Times: 0\n",
      "[10/100, 0/122] loss: 0.10495927\n",
      "[10/100, 100/122] loss: 0.084883362\n",
      "Current Macro F1: 63.38386432894257\n",
      "Trigger Times: 1\n",
      "[11/100, 0/122] loss: 0.054047454\n",
      "[11/100, 100/122] loss: 0.045769311\n",
      "Current Macro F1: 63.872971513378886\n",
      "Trigger Times: 0\n",
      "[12/100, 0/122] loss: 0.034525331\n",
      "[12/100, 100/122] loss: 0.05158297\n",
      "Current Macro F1: 62.44653772255578\n",
      "Trigger Times: 1\n",
      "[13/100, 0/122] loss: 0.058611259\n",
      "[13/100, 100/122] loss: 0.069631137\n",
      "Current Macro F1: 64.8912338723092\n",
      "Trigger Times: 0\n",
      "[14/100, 0/122] loss: 0.051684324\n",
      "[14/100, 100/122] loss: 0.020136628\n",
      "Current Macro F1: 64.85787519580512\n",
      "Trigger Times: 1\n",
      "[15/100, 0/122] loss: 0.027977355\n",
      "[15/100, 100/122] loss: 0.041225754\n",
      "Current Macro F1: 62.855757979336424\n",
      "Trigger Times: 2\n",
      "[16/100, 0/122] loss: 0.042331509\n",
      "[16/100, 100/122] loss: 0.038759112\n",
      "Current Macro F1: 64.4146781164749\n",
      "Trigger Times: 0\n",
      "[17/100, 0/122] loss: 0.064242385\n",
      "[17/100, 100/122] loss: 0.019724486\n",
      "Current Macro F1: 64.5966625966626\n",
      "Trigger Times: 0\n",
      "[18/100, 0/122] loss: 0.021007061\n",
      "[18/100, 100/122] loss: 0.021510327\n",
      "Current Macro F1: 64.60088996453916\n",
      "Trigger Times: 0\n",
      "[19/100, 0/122] loss: 0.0058160359\n",
      "[19/100, 100/122] loss: 0.0097380839\n",
      "Current Macro F1: 63.391673597274036\n",
      "Trigger Times: 1\n",
      "[20/100, 0/122] loss: 0.018023368\n",
      "[20/100, 100/122] loss: 0.083659835\n",
      "Current Macro F1: 65.17450357739281\n",
      "Trigger Times: 0\n",
      "[21/100, 0/122] loss: 0.014418127\n",
      "[21/100, 100/122] loss: 0.037038941\n",
      "Current Macro F1: 65.48386642399461\n",
      "Trigger Times: 0\n",
      "[22/100, 0/122] loss: 0.0032920705\n",
      "[22/100, 100/122] loss: 0.033944264\n",
      "Current Macro F1: 65.36278743813007\n",
      "Trigger Times: 1\n",
      "[23/100, 0/122] loss: 0.05522608\n",
      "[23/100, 100/122] loss: 0.064050548\n",
      "Current Macro F1: 65.06357617744264\n",
      "Trigger Times: 2\n",
      "[24/100, 0/122] loss: 0.17084856\n",
      "[24/100, 100/122] loss: 0.013224778\n",
      "Current Macro F1: 64.395875309188\n",
      "Trigger Times: 3\n",
      "Early stopping!\n",
      "Starting random seed # 123\n",
      "[0/100, 0/122] loss: 0.77075303\n",
      "[0/100, 100/122] loss: 0.63544375\n",
      "Current Macro F1: 66.86637372964812\n",
      "Trigger Times: 0\n",
      "[1/100, 0/122] loss: 0.31535566\n",
      "[1/100, 100/122] loss: 0.16858271\n",
      "Current Macro F1: 67.21180574989273\n",
      "Trigger Times: 0\n",
      "[2/100, 0/122] loss: 0.62933952\n",
      "[2/100, 100/122] loss: 0.33504957\n",
      "Current Macro F1: 69.25112790022719\n",
      "Trigger Times: 0\n",
      "[3/100, 0/122] loss: 0.36119795\n",
      "[3/100, 100/122] loss: 0.1201698\n",
      "Current Macro F1: 67.13277465819635\n",
      "Trigger Times: 1\n",
      "[4/100, 0/122] loss: 0.19933891\n",
      "[4/100, 100/122] loss: 0.12598786\n",
      "Current Macro F1: 67.51714833786546\n",
      "Trigger Times: 0\n",
      "[5/100, 0/122] loss: 0.28434023\n",
      "[5/100, 100/122] loss: 0.22787026\n",
      "Current Macro F1: 66.48610349528377\n",
      "Trigger Times: 1\n",
      "[6/100, 0/122] loss: 0.14864233\n",
      "[6/100, 100/122] loss: 0.12895027\n",
      "Current Macro F1: 63.95915442200272\n",
      "Trigger Times: 2\n",
      "[7/100, 0/122] loss: 0.17463054\n",
      "[7/100, 100/122] loss: 0.35717312\n",
      "Current Macro F1: 63.286497526572916\n",
      "Trigger Times: 3\n",
      "Early stopping!\n",
      "Starting random seed # 1234\n",
      "[0/100, 0/122] loss: 0.7076484\n",
      "[0/100, 100/122] loss: 0.26522282\n",
      "Current Macro F1: 66.71756154200862\n",
      "Trigger Times: 0\n",
      "[1/100, 0/122] loss: 0.29415944\n",
      "[1/100, 100/122] loss: 0.26702276\n",
      "Current Macro F1: 63.77182080973386\n",
      "Trigger Times: 1\n",
      "[2/100, 0/122] loss: 0.30270278\n",
      "[2/100, 100/122] loss: 0.13002311\n",
      "Current Macro F1: 64.92928367928369\n",
      "Trigger Times: 0\n",
      "[3/100, 0/122] loss: 0.20897722\n",
      "[3/100, 100/122] loss: 0.28892148\n",
      "Current Macro F1: 65.04264280468246\n",
      "Trigger Times: 0\n",
      "[4/100, 0/122] loss: 0.2301476\n",
      "[4/100, 100/122] loss: 0.33177412\n",
      "Current Macro F1: 65.80183705183704\n",
      "Trigger Times: 0\n",
      "[5/100, 0/122] loss: 0.14039262\n",
      "[5/100, 100/122] loss: 0.16730058\n",
      "Current Macro F1: 65.81197431021232\n",
      "Trigger Times: 0\n",
      "[6/100, 0/122] loss: 0.070218541\n",
      "[6/100, 100/122] loss: 0.12292131\n",
      "Current Macro F1: 63.69784162616961\n",
      "Trigger Times: 1\n",
      "[7/100, 0/122] loss: 0.17895302\n",
      "[7/100, 100/122] loss: 0.099439532\n",
      "Current Macro F1: 64.72636067514182\n",
      "Trigger Times: 0\n",
      "[8/100, 0/122] loss: 0.10098112\n",
      "[8/100, 100/122] loss: 0.12155243\n",
      "Current Macro F1: 64.94407338471987\n",
      "Trigger Times: 0\n",
      "[9/100, 0/122] loss: 0.090620168\n",
      "[9/100, 100/122] loss: 0.17203094\n",
      "Current Macro F1: 64.53411306042884\n",
      "Trigger Times: 1\n",
      "[10/100, 0/122] loss: 0.09475325\n",
      "[10/100, 100/122] loss: 0.073875546\n",
      "Current Macro F1: 63.16448473266182\n",
      "Trigger Times: 2\n",
      "[11/100, 0/122] loss: 0.06433817\n",
      "[11/100, 100/122] loss: 0.07368619\n",
      "Current Macro F1: 63.781619263634184\n",
      "Trigger Times: 0\n",
      "[12/100, 0/122] loss: 0.025092283\n",
      "[12/100, 100/122] loss: 0.04805344\n",
      "Current Macro F1: 65.10661652588983\n",
      "Trigger Times: 0\n",
      "[13/100, 0/122] loss: 0.070536017\n",
      "[13/100, 100/122] loss: 0.052743308\n",
      "Current Macro F1: 64.60379718625367\n",
      "Trigger Times: 1\n",
      "[14/100, 0/122] loss: 0.048056722\n",
      "[14/100, 100/122] loss: 0.064259425\n",
      "Current Macro F1: 64.75650272333374\n",
      "Trigger Times: 0\n",
      "[15/100, 0/122] loss: 0.10382716\n",
      "[15/100, 100/122] loss: 0.062974803\n",
      "Current Macro F1: 65.90126094596654\n",
      "Trigger Times: 0\n",
      "[16/100, 0/122] loss: 0.038367927\n",
      "[16/100, 100/122] loss: 0.06967172\n",
      "Current Macro F1: 66.37793255440315\n",
      "Trigger Times: 0\n",
      "[17/100, 0/122] loss: 0.078385018\n",
      "[17/100, 100/122] loss: 0.023486257\n",
      "Current Macro F1: 62.75896247402544\n",
      "Trigger Times: 1\n",
      "[18/100, 0/122] loss: 0.066766009\n",
      "[18/100, 100/122] loss: 0.026132083\n",
      "Current Macro F1: 63.34348898754584\n",
      "Trigger Times: 0\n",
      "[19/100, 0/122] loss: 0.036896858\n",
      "[19/100, 100/122] loss: 0.1063183\n",
      "Current Macro F1: 64.2220258761798\n",
      "Trigger Times: 0\n",
      "[20/100, 0/122] loss: 0.016433684\n",
      "[20/100, 100/122] loss: 0.016571077\n",
      "Current Macro F1: 65.08238335964772\n",
      "Trigger Times: 0\n",
      "[21/100, 0/122] loss: 0.025657557\n",
      "[21/100, 100/122] loss: 0.091425478\n",
      "Current Macro F1: 65.99179913020828\n",
      "Trigger Times: 0\n",
      "[22/100, 0/122] loss: 0.010271376\n",
      "[22/100, 100/122] loss: 0.02557826\n",
      "Current Macro F1: 63.66108528318388\n",
      "Trigger Times: 1\n",
      "[23/100, 0/122] loss: 0.032447629\n",
      "[23/100, 100/122] loss: 0.0088712247\n",
      "Current Macro F1: 62.501805428032974\n",
      "Trigger Times: 2\n",
      "[24/100, 0/122] loss: 0.029355928\n",
      "[24/100, 100/122] loss: 0.002162352\n",
      "Current Macro F1: 64.41310052630807\n",
      "Trigger Times: 0\n",
      "[25/100, 0/122] loss: 0.0030866978\n",
      "[25/100, 100/122] loss: 0.015117768\n",
      "Current Macro F1: 63.70095647957813\n",
      "Trigger Times: 1\n",
      "[26/100, 0/122] loss: 0.040041864\n",
      "[26/100, 100/122] loss: 0.02435923\n",
      "Current Macro F1: 63.66010613769103\n",
      "Trigger Times: 2\n",
      "[27/100, 0/122] loss: 0.1433716\n",
      "[27/100, 100/122] loss: 0.022502949\n",
      "Current Macro F1: 65.38170980097347\n",
      "Trigger Times: 0\n",
      "[28/100, 0/122] loss: 0.028493334\n",
      "[28/100, 100/122] loss: 0.011356306\n",
      "Current Macro F1: 65.2794118336382\n",
      "Trigger Times: 1\n",
      "[29/100, 0/122] loss: 0.012406121\n",
      "[29/100, 100/122] loss: 0.0031089159\n",
      "Current Macro F1: 64.05989528169296\n",
      "Trigger Times: 2\n",
      "[30/100, 0/122] loss: 0.0014933525\n",
      "[30/100, 100/122] loss: 0.0038306213\n",
      "Current Macro F1: 65.12519561815337\n",
      "Trigger Times: 0\n",
      "[31/100, 0/122] loss: 0.015755912\n",
      "[31/100, 100/122] loss: 0.0030495487\n",
      "Current Macro F1: 64.4535134628014\n",
      "Trigger Times: 1\n",
      "[32/100, 0/122] loss: 0.0017252387\n",
      "[32/100, 100/122] loss: 0.0017195081\n",
      "Current Macro F1: 64.18718183137794\n",
      "Trigger Times: 2\n",
      "[33/100, 0/122] loss: 0.0039100666\n",
      "[33/100, 100/122] loss: 0.022880955\n",
      "Current Macro F1: 62.58479059446621\n",
      "Trigger Times: 3\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from datetime import date\n",
    "from utils.classification_utils import set_seed, validation, training, testing\n",
    "from utils.loss_functions import FocalLoss, ClassBalanced_FocalLoss\n",
    "from models.bilstm_stacked import BiLSTM_stacked\n",
    "\n",
    "#Tuning over folds and random seeds\n",
    "ft_i = 0 #run number\n",
    "for lr in learning_rate:\n",
    "    for hid_d_lstm1 in hidden_dim_lstm1:\n",
    "        for dp in dropout_rate:\n",
    "            for b_size in BATCH_SIZE:\n",
    "                for g in gamma:\n",
    "                        #tuning parameters number\n",
    "                        str_version = 'tuning' + str(ft_i)\n",
    "                        ft_i+=1\n",
    "\n",
    "                        #dictionary of model parameters\n",
    "                        classifier_params = {\n",
    "                            \"num_epochs\": num_epochs,\n",
    "                            \"learning_rate\": lr,\n",
    "                            \"gamma\": g,\n",
    "                            \"beta\": beta,\n",
    "                            \"BATCH_SIZE\": b_size,\n",
    "                            \"NUM_folds\": NUM_folds,\n",
    "                            \"patience\": patience,\n",
    "                            \"loss\": loss,\n",
    "                            \"RANDOM_SEED_list\": RANDOM_SEED_list,\n",
    "                            \"embedding_dimensions\": embedding_dim,\n",
    "                            \"hidden_dim_lstm1\": hid_d_lstm1,\n",
    "                            \"hidden_dim_lstm2\" : hidden_dim_lstm2,\n",
    "                            \"output_dim\": output_dim,\n",
    "                            \"dropout_rate\": dp,\n",
    "                            \"pad_with\": pad_with\n",
    "                        }\n",
    "                                        \n",
    "                        for my_ran_seed in RANDOM_SEED_list:\n",
    "                            set_seed(my_ran_seed)\n",
    "                            myGenerator = torch.Generator()\n",
    "                            myGenerator.manual_seed(my_ran_seed)    \n",
    "                            for test_fold in range(NUM_folds):\n",
    "\n",
    "                                print('Starting random seed #',my_ran_seed)\n",
    "                                #get ith-fold data\n",
    "                                x_test, y_test, x_valid, y_valid, x_train , y_train, test_tl_ids, test_pids = Splits.get_reddit_splits(df, x_data, y_data)\n",
    "\n",
    "                                #data loaders with batches\n",
    "                                train = torch.utils.data.TensorDataset( x_train, y_train)\n",
    "                                valid = torch.utils.data.TensorDataset( x_valid, y_valid)\n",
    "                                test = torch.utils.data.TensorDataset( x_test, y_test)\n",
    "\n",
    "                                train_loader = torch.utils.data.DataLoader(dataset=train, batch_size = b_size, shuffle = True)\n",
    "                                valid_loader = torch.utils.data.DataLoader(dataset=valid, batch_size = b_size, shuffle = True)\n",
    "                                test_loader = torch.utils.data.DataLoader(dataset=test, batch_size = b_size, shuffle = True)\n",
    "\n",
    "                                #early stopping params\n",
    "                                last_metric = 0\n",
    "                                trigger_times = 0\n",
    "                                best_metric = 0\n",
    "\n",
    "                                #model\n",
    "                                model = BiLSTM_stacked(embedding_dim,\n",
    "                                        hid_d_lstm1,\n",
    "                                        hidden_dim_lstm2, \n",
    "                                        output_dim, \n",
    "                                        dp,\n",
    "                                        pad_with = pad_with)\n",
    "\n",
    "                                #loss function\n",
    "                                if (loss=='focal') :\n",
    "                                    alpha_values = torch.Tensor([math.sqrt(1/(y_train[y_train==0].shape[0]/y_train.shape[0])), math.sqrt(1/(y_train[y_train==1].shape[0]/y_train.shape[0])), math.sqrt(1/(y_train[y_train==2].shape[0]/y_train.shape[0]))])\n",
    "                                    criterion = FocalLoss(gamma = g, alpha = alpha_values)\n",
    "                                elif (loss == 'cbfocal'):\n",
    "                                    classifier_params[\"beta\"] = beta\n",
    "                                    samples_count = torch.Tensor([y_train[y_train==0].shape[0], y_train[y_train==1].shape[0], y_train[y_train==2].shape[0]])\n",
    "                                    criterion = ClassBalanced_FocalLoss(gamma = g, beta = beta, no_of_classes=3, samples_per_cls=samples_count)   \n",
    "                                elif (loss == 'cross_entropy'):\n",
    "                                    criterion = nn.CrossEntropyLoss()                            \n",
    "                                optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                                #model train/validation per epoch\n",
    "                                for epoch in range(num_epochs):\n",
    "\n",
    "                                    training(model, train_loader, criterion, optimizer, epoch, num_epochs)\n",
    "\n",
    "                                    # Early stopping\n",
    "                                    _ , f1_v, labels_val, predicted_val = validation(model, valid_loader, criterion, loss)\n",
    "\n",
    "                                    print('Current Macro F1:', f1_v)\n",
    "\n",
    "                                    if f1_v > best_metric :\n",
    "                                        best_metric = f1_v\n",
    "\n",
    "                                        #test and save so far best model\n",
    "                                        predicted_test, labels_test = testing(model, test_loader, loss)\n",
    "\n",
    "                                        results = {\n",
    "                                            \"model_code_name\": model_code_name, \n",
    "                                            \"model_specifics\": model_specifics, \n",
    "                                            \"classifier_params\": classifier_params, \n",
    "                                            \"date_run\": date.today().strftime(\"%d/%m/%Y\"),\n",
    "                                            \"test_tl_ids\": test_tl_ids,\n",
    "                                            \"test_pids\": test_pids,\n",
    "                                            \"labels\": labels_test,\n",
    "                                            \"predictions\": predicted_test,\n",
    "                                            \"labels_val\": labels_val,\n",
    "                                            \"predicted_val\": predicted_val,\n",
    "                                            \"test_fold\": test_fold,\n",
    "                                            \"random_seed\": my_ran_seed,\n",
    "                                            \"epoch\": epoch,\n",
    "                                        }\n",
    "\n",
    "                                        if (save_results==True):\n",
    "                                            file_name_results = paths.FOLDER_results + model_code_name + \"_\" + str(my_ran_seed) + \"seed\" + \"_\" + str_version + '.pkl'\n",
    "                                            file_name_model = paths.FOLDER_models + model_code_name + \"_\" + str(my_ran_seed) + \"seed\"  + \"_\" + str_version +'.pkl'\n",
    "                                            pickle.dump(results, open(file_name_results, 'wb'))\n",
    "                                            torch.save(model.state_dict(), file_name_model)\n",
    "\n",
    "                                    if f1_v < last_metric:\n",
    "                                        trigger_times += 1\n",
    "                                        print('Trigger Times:', trigger_times)\n",
    "\n",
    "                                        if trigger_times >= patience:\n",
    "                                            print('Early stopping!')\n",
    "                                            break\n",
    "\n",
    "                                    else:\n",
    "                                        print('Trigger Times: 0')\n",
    "                                        trigger_times = 0\n",
    "\n",
    "                                    last_metric = f1_v\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38-MoC",
   "language": "python",
   "name": "py38-moc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
