{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_pickle(paths.filename_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "#model specifics\n",
    "model_specifics = {\"data\": 'Reddit',\n",
    "    \"global_embedding_tp\": 'SBERT', #options: SBERT, BERT_cls , BERT_mean, BERT_max\n",
    "    \"dimensionality_reduction_tp\": 'umap', #options: ppapca, ppapcappa, umap\n",
    "    \"dimensionality_reduction_components\": 15, # options: any int number between 1 and embedding dimensions\n",
    "    \"dimensionality_reduction\": True, #options: True, False\n",
    "    \"time_injection_post_tp\": 'timestamp', #options: timestamp, None\n",
    "    \"signature_dimensions\": 3, #options: any int number larger than 1\n",
    "    \"post_embedding_tp\": 'sentence', #options: sentence, reduced, None\n",
    "    \"w\":5, #integer greater or equal to 2\n",
    "    \"k\":3, #integer smaller or equal to w\n",
    "    \"n\":9, #integer greater or equal to 2\n",
    "    \"loss_function\": 'focal', #options: focal, cbfocal\n",
    "    \"classifier_name\": 'SeqSigNet', #any string name\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6195, 384)\n"
     ]
    }
   ],
   "source": [
    "from utils.embeddings import Representations\n",
    "rep = Representations(type = model_specifics['global_embedding_tp'], filename =paths.filename_sbert)\n",
    "embeddings_sentence = rep.get_embeddings()\n",
    "\n",
    "print(embeddings_sentence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6195, 15)\n"
     ]
    }
   ],
   "source": [
    "from utils.dimensionality_reduction import DimensionalityReduction\n",
    "\n",
    "reduction = DimensionalityReduction(method= model_specifics['dimensionality_reduction_tp'], components=model_specifics['dimensionality_reduction_components'])\n",
    "embeddings_reduced = reduction.fit_transform(embeddings_sentence)\n",
    "\n",
    "print(embeddings_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenation with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import get_modeling_dataframe\n",
    "df = get_modeling_dataframe(annotations, embeddings_sentence, embeddings_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ttseriotou/signature_transforms/seq-sig-net/seq-sig-net/notebooks/../utils/timeinjection.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['timeline_index'][first_index:last_index] = np.arange(t_id_len)\n"
     ]
    }
   ],
   "source": [
    "from utils.timeinjection import TimeFeatures\n",
    "tf = TimeFeatures()\n",
    "df = tf.get_time_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6195, 400, 5, 9])\n"
     ]
    }
   ],
   "source": [
    "from utils.preparedata import PrepareData\n",
    "\n",
    "getdata = PrepareData(model_specifics, time_column = 'time_encoding', zero_padding=True, w_last=True)\n",
    "df, df_padded = getdata.pad(df)\n",
    "x_data = getdata.unit_input(df, df_padded)\n",
    "x_data = getdata.lstm_input(df, x_data)\n",
    "print(x_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.classification_utils import Splits\n",
    "\n",
    "NUM_folds = 1\n",
    "splits = Splits(num_folds=NUM_folds)\n",
    "y_data = splits.get_labels(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit_SBERT_umap15_timestamp_sentence_SeqSigNet\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "#TRAINING PARAMETERS\n",
    "num_epochs = 70\n",
    "learning_rate =  [0.0001] \n",
    "gamma = [2] \n",
    "beta = None\n",
    "BATCH_SIZE = 64\n",
    "NUM_folds = 1\n",
    "patience = 3\n",
    "loss = 'focal'\n",
    "weight_decay_adam = 0.0001\n",
    "RANDOM_SEED_list = [0, 1, 12, 123, 1234]\n",
    "# ================================\n",
    "# MODEL PARAMETERS\n",
    "input_channels = model_specifics['dimensionality_reduction_components'] #15\n",
    "output_channels = [10]\n",
    "sig_d = 3\n",
    "hidden_dim_lstm = [(10,200)]\n",
    "embedding_dim = embeddings_sentence.shape[1] #384\n",
    "hidden_dim= [64]\n",
    "output_dim = y_data.unique().size()[0] #3\n",
    "dropout_rate= [0.1]\n",
    "augmentation_tp =  'Conv1d'\n",
    "augmentation_layers = ()\n",
    "comb_method ='concatenation'\n",
    "attention = False \n",
    "# ================================\n",
    "# MODEL OPTIONS\n",
    "save_results = False\n",
    "\n",
    "if (model_specifics['dimensionality_reduction'] == True):\n",
    "    model_code_name = model_specifics[\"data\"] + \"_\" + model_specifics[\"global_embedding_tp\"]  \\\n",
    "    + \"_\" + str(model_specifics['dimensionality_reduction_tp']) + str(model_specifics['dimensionality_reduction_components']) \\\n",
    "    + \"_\" + str(model_specifics['time_injection_post_tp']) \\\n",
    "    + \"_\" + str(model_specifics['post_embedding_tp'])  \\\n",
    "    + \"_\" + str(model_specifics['classifier_name'])\n",
    "else:\n",
    "    model_code_name = model_specifics[\"data\"] + \"_\" + model_specifics[\"global_embedding_tp\"]  \\\n",
    "    + \"_\" + str(model_specifics['time_injection_post_tp']) \\\n",
    "    + \"_\" + str(model_specifics['post_embedding_tp'])  \\\n",
    "    + \"_\" + str(model_specifics['classifier_name'])\n",
    "\n",
    "print(model_code_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting random seed # 0\n",
      "[0/70, 0/61] loss: 0.79727864\n",
      "Current Macro F1: 30.46796256299496\n",
      "Trigger Times: 0\n",
      "[1/70, 0/61] loss: 0.77522522\n",
      "Current Macro F1: 30.46796256299496\n",
      "Trigger Times: 0\n",
      "[2/70, 0/61] loss: 0.72573131\n",
      "Current Macro F1: 35.42288184215979\n",
      "Trigger Times: 0\n",
      "[3/70, 0/61] loss: 0.5409655\n",
      "Current Macro F1: 50.39670953895642\n",
      "Trigger Times: 0\n",
      "[4/70, 0/61] loss: 0.55077696\n",
      "Current Macro F1: 51.96550514740773\n",
      "Trigger Times: 0\n",
      "[5/70, 0/61] loss: 0.4447684\n",
      "Current Macro F1: 51.090723751274204\n",
      "Trigger Times: 1\n",
      "[6/70, 0/61] loss: 0.23496766\n",
      "Current Macro F1: 48.59629082027047\n",
      "Trigger Times: 2\n",
      "[7/70, 0/61] loss: 0.49163908\n",
      "Current Macro F1: 50.714508200822195\n",
      "Trigger Times: 0\n",
      "[8/70, 0/61] loss: 0.43227866\n",
      "Current Macro F1: 50.912175512141346\n",
      "Trigger Times: 0\n",
      "[9/70, 0/61] loss: 0.24646279\n",
      "Current Macro F1: 57.42363852353041\n",
      "Trigger Times: 0\n",
      "[10/70, 0/61] loss: 0.40837479\n",
      "Current Macro F1: 59.2370940885878\n",
      "Trigger Times: 0\n",
      "[11/70, 0/61] loss: 0.49314445\n",
      "Current Macro F1: 63.52598588725993\n",
      "Trigger Times: 0\n",
      "[12/70, 0/61] loss: 0.37017754\n",
      "Current Macro F1: 69.10307911438137\n",
      "Trigger Times: 0\n",
      "[13/70, 0/61] loss: 0.40881553\n",
      "Current Macro F1: 71.07293401722234\n",
      "Trigger Times: 0\n",
      "[14/70, 0/61] loss: 0.31815964\n",
      "Current Macro F1: 68.95984785424801\n",
      "Trigger Times: 1\n",
      "[15/70, 0/61] loss: 0.46020812\n",
      "Current Macro F1: 71.94587475926105\n",
      "Trigger Times: 0\n",
      "[16/70, 0/61] loss: 0.34840989\n",
      "Current Macro F1: 69.84386703320676\n",
      "Trigger Times: 1\n",
      "[17/70, 0/61] loss: 0.31578979\n",
      "Current Macro F1: 70.09035605653963\n",
      "Trigger Times: 0\n",
      "[18/70, 0/61] loss: 0.33789724\n",
      "Current Macro F1: 73.35549249454203\n",
      "Trigger Times: 0\n",
      "[19/70, 0/61] loss: 0.32641545\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from datetime import date\n",
    "from utils.classification_utils import set_seed, validation, training, testing\n",
    "from utils.loss_functions import FocalLoss, ClassBalanced_FocalLoss\n",
    "from models.seqsignet import SeqSigNet\n",
    "\n",
    "#Tuning over folds and random seeds\n",
    "ft_i = 0 #run number\n",
    "for out_ch in output_channels:\n",
    "    for lr in learning_rate:\n",
    "        for g in gamma:\n",
    "            for dp in dropout_rate:\n",
    "                for h_dim in hidden_dim:\n",
    "                    for lstm_dim in hidden_dim_lstm:\n",
    "                        #tuning parameters number\n",
    "                        str_version = 'tuning' + str(ft_i)\n",
    "                        ft_i+=1\n",
    "\n",
    "                        #dictionary of model parameters\n",
    "                        classifier_params = {\n",
    "                            \"num_epochs\": num_epochs,\n",
    "                            \"learning_rate\": lr,\n",
    "                            \"gamma\": g,\n",
    "                            \"beta\": beta,\n",
    "                            \"BATCH_SIZE\": BATCH_SIZE,\n",
    "                            \"NUM_folds\": NUM_folds,\n",
    "                            \"patience\": patience,\n",
    "                            \"loss\": loss,\n",
    "                            \"weight_decay_adam\": weight_decay_adam,\n",
    "                            \"RANDOM_SEED_list\": RANDOM_SEED_list,\n",
    "                            \"input_channels\": input_channels,\n",
    "                            \"output_channels\": out_ch,\n",
    "                            \"sig_d\": sig_d,\n",
    "                            \"hidden_dim_lstm\": lstm_dim,\n",
    "                            \"embedding_dimensions\": embedding_dim,\n",
    "                            \"hidden_dim\": h_dim,\n",
    "                            \"output_dim\": output_dim,\n",
    "                            \"dropout_rate\": dp,\n",
    "                            \"augmentation_tp\": augmentation_tp,\n",
    "                            \"augmentation_layers\": augmentation_layers,\n",
    "                            \"combination_method\": comb_method,\n",
    "                            \"attention\": attention\n",
    "                        }\n",
    "                                        \n",
    "                        for my_ran_seed in RANDOM_SEED_list:\n",
    "                            set_seed(my_ran_seed)\n",
    "                            myGenerator = torch.Generator()\n",
    "                            myGenerator.manual_seed(my_ran_seed)    \n",
    "                            for test_fold in range(NUM_folds):\n",
    "\n",
    "                                print('Starting random seed #',my_ran_seed)\n",
    "                                #get ith-fold data\n",
    "                                x_test, y_test, x_valid, y_valid, x_train , y_train, test_tl_ids, test_pids = Splits.get_reddit_splits(df, x_data, y_data)\n",
    "\n",
    "                                #data loaders with batches\n",
    "                                train = torch.utils.data.TensorDataset( x_train, y_train)\n",
    "                                valid = torch.utils.data.TensorDataset( x_valid, y_valid)\n",
    "                                test = torch.utils.data.TensorDataset( x_test, y_test)\n",
    "\n",
    "                                train_loader = torch.utils.data.DataLoader(dataset=train, batch_size = BATCH_SIZE, shuffle = True)\n",
    "                                valid_loader = torch.utils.data.DataLoader(dataset=valid, batch_size = BATCH_SIZE, shuffle = True)\n",
    "                                test_loader = torch.utils.data.DataLoader(dataset=test, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "                                #early stopping params\n",
    "                                last_metric = 0\n",
    "                                trigger_times = 0\n",
    "                                best_metric = 0\n",
    "\n",
    "                                #model\n",
    "                                model = SeqSigNet(input_channels, \n",
    "                                            out_ch, \n",
    "                                            sig_d, \n",
    "                                            lstm_dim,\n",
    "                                            embedding_dim,\n",
    "                                            h_dim,\n",
    "                                            output_dim,\n",
    "                                            dp, \n",
    "                                            augmentation_tp,\n",
    "                                            augmentation_layers,\n",
    "                                            comb_method, \n",
    "                                            attention)\n",
    "\n",
    "                                #loss function\n",
    "                                if (loss=='focal') :\n",
    "                                    alpha_values = torch.Tensor([math.sqrt(1/(y_train[y_train==0].shape[0]/y_train.shape[0])), math.sqrt(1/(y_train[y_train==1].shape[0]/y_train.shape[0])), math.sqrt(1/(y_train[y_train==2].shape[0]/y_train.shape[0]))])\n",
    "                                    criterion = FocalLoss(gamma = g, alpha = alpha_values)\n",
    "                                elif (loss == 'cbfocal'):\n",
    "                                    classifier_params[\"beta\"] = beta\n",
    "                                    samples_count = torch.Tensor([y_train[y_train==0].shape[0], y_train[y_train==1].shape[0], y_train[y_train==2].shape[0]])\n",
    "                                    criterion = ClassBalanced_FocalLoss(gamma = g, beta = beta, no_of_classes=3, samples_per_cls=samples_count)   \n",
    "                                elif (loss == 'cross_entropy'):\n",
    "                                    criterion = nn.CrossEntropyLoss()                            \n",
    "                                optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay= weight_decay_adam)\n",
    "\n",
    "                                #model train/validation per epoch\n",
    "                                for epoch in range(num_epochs):\n",
    "\n",
    "                                    training(model, train_loader, criterion, optimizer, epoch, num_epochs)\n",
    "\n",
    "                                    # Early stopping\n",
    "                                    _ , f1_v, labels_val, predicted_val = validation(model, valid_loader, criterion, loss)\n",
    "\n",
    "                                    print('Current Macro F1:', f1_v)\n",
    "\n",
    "                                    if f1_v > best_metric :\n",
    "                                        best_metric = f1_v\n",
    "\n",
    "                                        #test and save so far best model\n",
    "                                        predicted_test, labels_test = testing(model, test_loader, loss)\n",
    "\n",
    "                                        results = {\n",
    "                                            \"model_code_name\": model_code_name, \n",
    "                                            \"model_specifics\": model_specifics, \n",
    "                                            \"classifier_params\": classifier_params, \n",
    "                                            \"date_run\": date.today().strftime(\"%d/%m/%Y\"),\n",
    "                                            \"test_tl_ids\": test_tl_ids,\n",
    "                                            \"test_pids\": test_pids,\n",
    "                                            \"labels\": labels_test,\n",
    "                                            \"predictions\": predicted_test,\n",
    "                                            \"labels_val\": labels_val,\n",
    "                                            \"predicted_val\": predicted_val,\n",
    "                                            \"test_fold\": test_fold,\n",
    "                                            \"random_seed\": my_ran_seed,\n",
    "                                            \"epoch\": epoch,\n",
    "                                        }\n",
    "\n",
    "                                        if (save_results==True):\n",
    "                                            file_name_results = paths.FOLDER_results + model_code_name + \"_\" + str(my_ran_seed) + \"seed\" + \"_\" + str_version + '.pkl'\n",
    "                                            file_name_model = paths.FOLDER_models + model_code_name + \"_\" + str(my_ran_seed) + \"seed\"  + \"_\" + str_version +'.pkl'\n",
    "                                            pickle.dump(results, open(file_name_results, 'wb'))\n",
    "                                            torch.save(model.state_dict(), file_name_model)\n",
    "\n",
    "                                    if f1_v < last_metric:\n",
    "                                        trigger_times += 1\n",
    "                                        print('Trigger Times:', trigger_times)\n",
    "\n",
    "                                        if trigger_times >= patience:\n",
    "                                            print('Early stopping!')\n",
    "                                            break\n",
    "\n",
    "                                    else:\n",
    "                                        print('Trigger Times: 0')\n",
    "                                        trigger_times = 0\n",
    "\n",
    "                                    last_metric = f1_v\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38-MoC",
   "language": "python",
   "name": "py38-moc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
